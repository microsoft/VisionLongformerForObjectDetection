MODEL:
  META_ARCHITECTURE: "RetinaNet"
  WEIGHTS: "msvit_pretrain.pth"
  MASK_ON: False
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  BACKBONE:
    NAME: "build_retinanet_msvit_fpn_backbone"
  TRANSFORMER:
    DROP: 0.0
    DROP_PATH: 0.0
    MSVIT:
      ARCH: l1,h3,d96,n1,s1,g1,p4,f7,a0_l2,h3,d192,n2,s1,g1,p2,f7,a0_l3,h6,d384,n8,s1,g1,p2,f7,a0_l4,h12,d768,n1,s1,g0,p2,f7,a0
      ATTN_TYPE: longformerhand
      ONLY_GLOBAL: False
      SHARE_KV: True
      SHARE_W: True
      SW_EXACT: 0
    NORM_EMBED: True
    OUT_FEATURES: ["layer2", "layer3", "layer4"]
  FPN:
    IN_FEATURES: ["layer2", "layer3", "layer4"]
  ANCHOR_GENERATOR:
    SIZES: !!python/object/apply:eval ["[[x, x * 2**(1.0/3), x * 2**(2.0/3) ] for x in [32, 64, 128, 256, 512 ]]"]
  RETINANET:
    IOU_THRESHOLDS: [0.4, 0.5]
    IOU_LABELS: [0, -1, 1]
    SMOOTH_L1_LOSS_BETA: 0.0
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.0001
  STEPS: (60000, 80000)
  MAX_ITER: 90000
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 1000
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  LR_MULTIPLIERS:
    backbone.bottom_up: 1.0
    backbone.bottom_up.patch_embed: 1.0
    backbone.bottom_up.x_pos_embed: 1.0
    backbone.bottom_up.y_pos_embed: 1.0
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 1.0
    NORM_TYPE: 2.0
INPUT:
#  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  CROP:
    ENABLED: False
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
TEST:
  EVAL_PERIOD: 7330
VERSION: 2