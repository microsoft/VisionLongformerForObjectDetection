MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHTS: "msvit_pretrain.pth"
  MASK_ON: True
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]  
  BACKBONE:
    NAME: "build_msvit_fpn_backbone"
  TRANSFORMER:
    DROP: 0.0
    DROP_PATH: 0.0
    MSVIT:
      # ARCH: l1,h1,d64,n1,s1,g1,p4,f7_l2,h3,d192,n1,s1,g1,p2,f7_l3,h6,d384,n10,s1,g1,p2,f7_l4,h6,d384,n1,s1,g0,p2,f7      
      # ATTN_TYPE: longformersw
      # NUM_FEATS: 256
      # ONLY_GLOBAL: False
      # SHARE_KV: True
      # SHARE_W: True
      # SW_EXACT: False
      # WIN_SIZE: 7
      ARCH: l1,h1,d48,n1,s1,g1,p4,f7,a0_l2,h3,d96,n1,s1,g1,p2,f7,a0_l3,h3,d192,n9,s1,g1,p2,f7,a0_l4,h6,d384,n1,s1,g0,p2,f7,a0
      ATTN_TYPE: longformerhand
      ONLY_GLOBAL: False
      SHARE_KV: True
      SHARE_W: True
      SW_EXACT: 0
    NORM_EMBED: True
    OUT_FEATURES: ["layer1", "layer2", "layer3", "layer4"]
  FPN:
    IN_FEATURES: ["layer1", "layer2", "layer3", "layer4"]
  ANCHOR_GENERATOR:
    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)
  RPN:
    IN_FEATURES: ["p2", "p3", "p4", "p5", "p6"]
    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level
    PRE_NMS_TOPK_TEST: 1000  # Per FPN level
    # Detectron1 uses 2000 proposals per-batch,
    # (See "modeling/rpn/rpn_outputs.py" for details of this legacy issue)
    # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.
    POST_NMS_TOPK_TRAIN: 1000
    POST_NMS_TOPK_TEST: 1000
  ROI_HEADS:
    NAME: "StandardROIHeads"
    IN_FEATURES: ["p2", "p3", "p4", "p5"]
  ROI_BOX_HEAD:
    NAME: "FastRCNNConvFCHead"
    NUM_FC: 2
    POOLER_RESOLUTION: 7
  ROI_MASK_HEAD:
    NAME: "MaskRCNNConvUpsampleHead"
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.00025
  STEPS: (60000, 80000)
  MAX_ITER: 90000
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 1000
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  LR_MULTIPLIERS:
    backbone.bottom_up: 1.0
    backbone.bottom_up.patch_embed: 1.0
    backbone.bottom_up.x_pos_embed: 1.0
    backbone.bottom_up.y_pos_embed: 1.0
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.5 # 1.0
    NORM_TYPE: 2.0
INPUT:
  # MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN: (800,)
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MAX_SIZE_TEST: 1333
  CROP:
    ENABLED: False
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
VERSION: 2